{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **MODELING and EVALUATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Business Requirement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Address Business Requirement 2: Develop a classification model to detect powdery mildew on cherry leaves.\n",
        "- Automate disease detection to save time and improve efficiency in plantation management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## **Objectives**\n",
        "\n",
        "- Baseline CNN Model: Train a simple CNN to classify cherry leaf images.\n",
        "- Regularization Techniques: Use L2 regularization and Dropout to prevent overfitting.\n",
        "- Hyperparameter Tuning: Optimize CNN performance using Keras Tuner.\n",
        "- Transfer Learning: Implement MobileNetV2 to compare performance with the CNN.\n",
        "- Model Evaluation & Explainability: Use Grad-CAM and feature visualization for model interpretation.\n",
        " \n",
        "\n",
        "## **Inputs**\n",
        "\n",
        "Dataset Directories:\n",
        "\n",
        "- inputs/mildew_dataset_dataset/cherry-leaves/train\n",
        "- inputs/mildew_dataset_dataset/cherry-leaves/test\n",
        "- inputs/mildew_dataset_dataset/cherry-leaves/validation\n",
        "\n",
        "Precomputed Features (from Data Visualization Notebook):\n",
        "- Image Shape Embeddings → Standardized to 128x128x3 for model consistency.\n",
        "- Class Distribution Insights → Confirmed dataset balance across train, validation, and test sets.\n",
        "- Feature Space Analysis → PCA and t-SNE confirm strong class separability.\n",
        "- Pixel Intensity Distribution → Brightness variations provide useful classification features.\n",
        "\n",
        "## **Outputs**\n",
        "\n",
        "- Visualizations & Preprocessing\n",
        "    - Image Distribution Plot → Confirms class balance in dataset.\n",
        "\t- Augmentation Visualization → Shows the impact of real-time transformations on training data.\n",
        "\n",
        "- Model Training & Optimization\n",
        "\t- CNN Model with Regularization → Designed to prevent overfitting.\n",
        "\t- Hyperparameter Tuning with GridSearchCV → Finds the best configuration.\n",
        "\t- Best Model Selection → Based on cross-validation performance.\n",
        "\t- Saved Trained Model → Ready for future inference and deployment.\n",
        "\n",
        "- Evaluation & Interpretability\n",
        "\t- Learning Curve Plot → Illustrates training progression over epochs.\n",
        "\t- Classification Report (Accuracy, Precision, Recall, F1-score) → Provides detailed performance metrics.\n",
        "\t- Confusion Matrix Visualization → Highlights classification strengths & weaknesses.\n",
        "\t- Grad-CAM & Occlusion Sensitivity Analysis → Explains model predictions visually.\n",
        "\n",
        "\n",
        "## **Additional Comments**\n",
        "\n",
        "- Business Impact: This notebook produces a robust mildew detection model for real-time classification.\n",
        "- Data-Driven Enhancements: Adjustments to augmentation, preprocessing, and model tuning are guided by insights from Data Visualization Notebook.\n",
        "- Deployment-Ready: The trained model is optimized for integration into a Streamlit application, ensuring real-world usability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Setup & Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "### **Import Necessary Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Set Working Directory & File Paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cwd= os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir('/workspace/mildew-detection-app')\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "work_dir = os.getcwd()\n",
        "work_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "### Set Input Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set train, validation and test paths\n",
        "my_data_dir = 'inputs/mildew_dataset/cherry-leaves'\n",
        "train_path = my_data_dir + '/train'\n",
        "val_path = my_data_dir + '/validation'\n",
        "test_path = my_data_dir + '/test'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Output Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "version = 'v1'\n",
        "file_path = f'outputs/{version}'\n",
        "\n",
        "if 'outputs' in os.listdir(work_dir) and version in os.listdir(work_dir + '/outputs'):\n",
        "    print('Old version is already available create a new version.')\n",
        "    pass\n",
        "else:\n",
        "    os.makedirs(name=file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Load Dataset & Labels**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the lables for the images\n",
        "labels = os.listdir(train_path)\n",
        "\n",
        "print(\n",
        "    f\"Project Labels: {labels}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Precomputed Image Shape from Data Visualization Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Import saved image shape embedding\n",
        "import joblib\n",
        "version = 'v1'\n",
        "image_shape = joblib.load(filename=f\"outputs/{version}/image_shape.pkl\")\n",
        "image_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Count and Visualize Dataset Distribution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Count Images in Each Dataset Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# Create an empty dictionary\n",
        "data = {\n",
        "    'Set': [],\n",
        "    'Label': [],\n",
        "    'Frequency': []\n",
        "}\n",
        "\n",
        "# Define dataset folders\n",
        "folders = ['train', 'validation', 'test']\n",
        "\n",
        "# Loop through each dataset split and count images\n",
        "for folder in folders:\n",
        "    for label in labels:\n",
        "        path = os.path.join(my_data_dir, folder, label)\n",
        "        num_images = len(os.listdir(path)) if os.path.exists(path) else 0  # Handle missing directories\n",
        "        data['Set'].append(folder)\n",
        "        data['Label'].append(label)\n",
        "        data['Frequency'].append(num_images)\n",
        "        print(f\" {folder}/{label}: {num_images} images\")\n",
        "\n",
        "# Convert dictionary to DataFrame\n",
        "df_freq = pd.DataFrame(data)\n",
        "\n",
        "# ** Bar Chart - Image Distribution**\n",
        "print(f\"Bar Chart: Image distribution across dataset splits.\")\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=df_freq, x='Set', y='Frequency', hue='Label')\n",
        "plt.title(\"Image Distribution in Dataset\")\n",
        "plt.xlabel(\"Dataset Split\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.savefig(f'{file_path}/labels_distribution.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# ** Pie Chart - Class Distribution**\n",
        "print(f\"Pie Chart: Overall class distribution (Healthy vs Infected).\")\n",
        "plt.figure(figsize=(5, 5))\n",
        "label_distribution = df_freq.groupby(\"Label\")[\"Frequency\"].sum()\n",
        "plt.pie(label_distribution, labels=label_distribution.index, autopct='%1.1f%%', startangle=90, colors=[\"#1f77b4\", \"#ff7f0e\"])\n",
        "plt.title(\"Overall Class Distribution (Healthy vs Infected)\")\n",
        "plt.savefig(f'{file_path}/labels_pie_chart.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Implement Data Augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Define Augmentation Techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rotation, width/height shift, zoom, shear, horizontal/vertical flip, rescaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import TensorFlow/Keras ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Augment Training, Validation, and Test Sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "### Initialize ImageDataGenerator for Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Augmentation for Training Set\n",
        "augmented_image_data = ImageDataGenerator(rotation_range=20,\n",
        "                                          width_shift_range=0.10,\n",
        "                                          height_shift_range=0.10,\n",
        "                                          shear_range=0.1,\n",
        "                                          zoom_range=0.1,\n",
        "                                          horizontal_flip=True,\n",
        "                                          vertical_flip=True,\n",
        "                                          fill_mode='nearest',\n",
        "                                          rescale=1./255\n",
        "                                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Augment Training Image Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 20  # Set batch size\n",
        "train_set = augmented_image_data.flow_from_directory(train_path,\n",
        "                                                     target_size=image_shape[:2],\n",
        "                                                     color_mode='rgb',\n",
        "                                                     batch_size=batch_size,\n",
        "                                                     class_mode='binary',\n",
        "                                                     shuffle=True\n",
        "                                                     )\n",
        "\n",
        "train_set.class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Augment Validation Image Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_set = ImageDataGenerator(rescale=1./255).flow_from_directory(val_path,\n",
        "                                                                        target_size=image_shape[:2],\n",
        "                                                                        color_mode='rgb',\n",
        "                                                                        batch_size=batch_size,\n",
        "                                                                        class_mode='binary',\n",
        "                                                                        shuffle=False\n",
        "                                                                        )\n",
        "\n",
        "validation_set.class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Augment Test Image Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set = ImageDataGenerator(rescale=1./255).flow_from_directory(test_path,\n",
        "                                                                  target_size=image_shape[:2],\n",
        "                                                                  color_mode='rgb',\n",
        "                                                                  batch_size=batch_size,\n",
        "                                                                  class_mode='binary',\n",
        "                                                                  shuffle=False\n",
        "                                                                  )\n",
        "\n",
        "test_set.class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Visualization of Augmented Images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot Augmented Training Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(3):\n",
        "    img, label = next(train_set)\n",
        "    print(img.shape)  # (1,256,256,3)\n",
        "    plt.imshow(img[0])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot Augmented Validation and Test Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(3):\n",
        "    img, label = next(validation_set)\n",
        "    print(img.shape)  # (1,256,256,3)\n",
        "    plt.imshow(img[0])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(3):\n",
        "    img, label = next(test_set)\n",
        "    print(img.shape)  # (1,256,256,3)\n",
        "    plt.imshow(img[0])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Class Indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=train_set.class_indices,\n",
        "            filename=f\"{file_path}/class_indices.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare Multiple Augmented Images in a Grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_augmented_images_grid(data_generator, num_images=10):\n",
        "    \"\"\"Displays a grid of augmented images to visualize transformation effects.\"\"\"\n",
        "    img_batch, label_batch = next(data_generator)\n",
        "\n",
        "    fig, axes = plt.subplots(2, num_images // 2, figsize=(15, 6))\n",
        "    \n",
        "    for i in range(num_images):\n",
        "        ax = axes[i // (num_images // 2), i % (num_images // 2)]\n",
        "        ax.imshow(img_batch[i])\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(\"Augmented Image Variations (Training Set)\")\n",
        "    plt.show()\n",
        "\n",
        "# Display the augmented image grid\n",
        "plot_augmented_images_grid(train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Model Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Model Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Base Convolutional Neural Network (CNN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The **Convolutional Neural Network (CNN)** is designed to classify cherry leaf images as **Healthy or Infected with Powdery Mildew**.\n",
        "\n",
        "Why this approach?\n",
        "- Feature Extraction: Uses four convolutional layers to detect patterns in leaf images.\n",
        "- Overfitting Prevention:\n",
        "  - L2 Regularization penalizes large weights, improving generalization.\n",
        "  - Dropout (0.5) randomly disables neurons, reducing overfitting.\n",
        "- Binary Classification:\n",
        "\t- Uses Sigmoid activation in the final layer for probability-based classification.\n",
        "- Efficient Optimization:\n",
        "\t- Adam optimizer (LR = 0.0001) ensures stable training.\n",
        "\n",
        "This base CNN serves as the foundation for further optimizations, such as hyperparameter tuning and transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the function to create the base CNN model\n",
        "def create_base_cnn():\n",
        "    \"\"\"\n",
        "    Creates a Convolutional Neural Network (CNN) model for binary classification.\n",
        "\n",
        "    The model consists of:\n",
        "    - Convolutional layers with ReLU activation and L2 regularization\n",
        "    - MaxPooling layers for downsampling\n",
        "    - Fully connected Dense layers with Dropout for regularization\n",
        "    - Sigmoid activation for binary classification (Healthy vs. Infected)\n",
        "\n",
        "    Returns:\n",
        "        model: A compiled Keras CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential(\n",
        "        [\n",
        "            # First convolutional block\n",
        "            Conv2D(\n",
        "                filters=32,\n",
        "                kernel_size=(3, 3),\n",
        "                activation=\"relu\",\n",
        "                input_shape=image_shape,\n",
        "                kernel_regularizer=l2(0.001),\n",
        "            ),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "            # Second convolutional block\n",
        "            Conv2D(\n",
        "                filters=64,\n",
        "                kernel_size=(3, 3),\n",
        "                activation=\"relu\",\n",
        "                kernel_regularizer=l2(0.001),\n",
        "            ),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "            # Third convolutional block\n",
        "            Conv2D(\n",
        "                filters=128,\n",
        "                kernel_size=(3, 3),\n",
        "                activation=\"relu\",\n",
        "                kernel_regularizer=l2(0.001),\n",
        "            ),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "            # Fourth convolutional block\n",
        "            Conv2D(\n",
        "                filters=128,\n",
        "                kernel_size=(3, 3),\n",
        "                activation=\"relu\",\n",
        "                kernel_regularizer=l2(0.001),\n",
        "            ),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "            # Flatten the feature maps into a single vector\n",
        "            Flatten(),\n",
        "            # Fully connected layers\n",
        "            Dense(128, activation=\"relu\", kernel_regularizer=l2(0.001)),\n",
        "            Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "            Dense(1, activation=\"sigmoid\"),  # Binary classification\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Compile Model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0001),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Train CNN Model with Early Stopping**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement EarlyStopping to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the CNN model for 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the CNN model\n",
        "model = create_base_cnn()\n",
        "model.fit(train_set,\n",
        "          epochs=20,\n",
        "          steps_per_epoch=len(train_set.classes) // batch_size,\n",
        "          validation_data=validation_set,\n",
        "          \n",
        "          callbacks=[early_stop],\n",
        "          verbose=1\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and Save the CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('outputs/v1/mildew_detector_cnn.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Model Performance & Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Evaluate CNN on the Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_set)\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\" Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\" Test Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plot Learning Curves**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert training history to DataFrame\n",
        "losses = pd.DataFrame(history.history) \n",
        "\n",
        "# Set Seaborn style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plot Loss Curve\n",
        "losses[[\"loss\", \"val_loss\"]].plot(style=\".-\")\n",
        "plt.title(\"Loss\")\n",
        "plt.savefig(f\"{file_path}/model_training_losses.png\", bbox_inches=\"tight\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Plot Accuracy Curve\n",
        "losses[[\"accuracy\", \"val_accuracy\"]].plot(style=\".-\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.savefig(f\"{file_path}/model_training_acc.png\", bbox_inches=\"tight\", dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Confusion Matrix & Classification Report**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"outputs/v1\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def interactive_confusion_matrix(y_true, y_pred, label_map, set_name):\n",
        "    \"\"\"\n",
        "    Generate an interactive confusion matrix using Plotly and save the image.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm_df = pd.DataFrame(cm, index=label_map, columns=label_map)\n",
        "\n",
        "    fig = px.imshow(\n",
        "        cm_df,\n",
        "        labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "        x=label_map,\n",
        "        y=label_map,\n",
        "        color_continuous_scale=\"Blues\",\n",
        "        text_auto=True,\n",
        "    )\n",
        "\n",
        "    fig.update_layout(title=f\"Confusion Matrix - {set_name} Set\")\n",
        "    fig.show()\n",
        "\n",
        "    # Save the interactive confusion matrix as an image\n",
        "    save_path = os.path.join(output_dir, f\"confusion_matrix_{set_name.lower()}.png\")\n",
        "    fig.write_image(save_path)\n",
        "    print(f\"Confusion Matrix saved: {save_path}\")\n",
        "\n",
        "    return cm\n",
        "\n",
        "\n",
        "def generate_classification_report(y_true, y_pred, label_map, set_name):\n",
        "    \"\"\"\n",
        "    Generate and print a classification report.\n",
        "    \"\"\"\n",
        "    report = classification_report(\n",
        "        y_true, y_pred, target_names=label_map, output_dict=True\n",
        "    )\n",
        "    report_str = classification_report(y_true, y_pred, target_names=label_map)\n",
        "\n",
        "    print(f\"--- Classification Report: {set_name} Set ---\")\n",
        "    print(report_str, \"\\n\")\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "def confusion_matrix_and_report(generator, model, label_map, set_name, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Generate an interactive confusion matrix and classification report.\n",
        "    \"\"\"\n",
        "    y_true = generator.classes\n",
        "    y_pred_probs = model.predict(generator)\n",
        "    y_pred = (y_pred_probs > threshold).astype(int).flatten()\n",
        "\n",
        "    print(f\"#### {set_name} Set ####\\n\")\n",
        "\n",
        "    cm = interactive_confusion_matrix(y_true, y_pred, label_map, set_name)\n",
        "    report = generate_classification_report(y_true, y_pred, label_map, set_name)\n",
        "\n",
        "    return cm, report\n",
        "\n",
        "\n",
        "def clf_performance(train_gen, val_gen, test_gen, model, label_map, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on train, validation, and test sets interactively.\n",
        "    \"\"\"\n",
        "    sets = {\"Train\": train_gen, \"Validation\": val_gen, \"Test\": test_gen}\n",
        "    reports = {}\n",
        "\n",
        "    for set_name, generator in sets.items():\n",
        "        cm, report = confusion_matrix_and_report(\n",
        "            generator, model, label_map, set_name, threshold\n",
        "        )\n",
        "        reports[set_name.lower()] = report\n",
        "\n",
        "    # Save classification reports as JSON\n",
        "    json_save_path = os.path.join(output_dir, \"classification_reports.json\")\n",
        "    with open(json_save_path, \"w\") as f:\n",
        "        json.dump(reports, f, indent=4)\n",
        "\n",
        "    print(f\"\\n Classification Reports saved: {json_save_path}\")\n",
        "    return reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create label map from training set class indices\n",
        "label_map = list(train_set.class_indices.keys())\n",
        "\n",
        "# Call the function with the label map\n",
        "reports = clf_performance(train_set, validation_set, test_set, model, label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **McNemar’s Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "McNemar's test is a statistical method used to compare two classification models on the same dataset. Unlike accuracy alone, this test evaluates whether the differences in their predictions are **statistically significant** or if they could have occurred by chance.\n",
        "\n",
        "It works by analyzing **misclassified instances**, specifically cases where one model gets a prediction right while the other gets it wrong. This helps determine if one model is **truly better** than the other, beyond just accuracy scores.\n",
        "\n",
        "A **p-value < 0.05** suggests that the models perform **significantly differently**, whereas a **p-value ≥ 0.05** means their performance is likely similar.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "\n",
        "\n",
        "def mcnemar_test(test_set, model_1, model_2):\n",
        "    \"\"\"\n",
        "    Perform McNemar's test to compare two classification models.\n",
        "    \"\"\"\n",
        "    # Extract ground-truth labels\n",
        "    y_true = test_set.classes  # True labels\n",
        "\n",
        "    # Get predictions from both models\n",
        "    y_pred_model_1 = (model_1.predict(test_set) > 0.5).astype(int).flatten()\n",
        "    y_pred_model_2 = (model_2.predict(test_set) > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Compute contingency table\n",
        "    a = np.sum((y_pred_model_1 == y_true) & (y_pred_model_2 == y_true))  # Both correct\n",
        "    b = np.sum(\n",
        "        (y_pred_model_1 == y_true) & (y_pred_model_2 != y_true)\n",
        "    )  # Model 1 correct, Model 2 wrong\n",
        "    c = np.sum(\n",
        "        (y_pred_model_1 != y_true) & (y_pred_model_2 == y_true)\n",
        "    )  # Model 1 wrong, Model 2 correct\n",
        "    d = np.sum((y_pred_model_1 != y_true) & (y_pred_model_2 != y_true))  # Both wrong\n",
        "\n",
        "    contingency_table = np.array([[a, b], [c, d]])\n",
        "    print(\"McNemar's Contingency Table:\\n\", contingency_table)\n",
        "\n",
        "    # Perform McNemar's test\n",
        "    result = mcnemar(contingency_table, exact=False)\n",
        "    print(f\"Chi-square statistic: {result.statistic}\")\n",
        "    print(f\"P-value: {result.pvalue}\")\n",
        "\n",
        "    # Interpretation\n",
        "    if result.pvalue < 0.05:\n",
        "        print(\"Significant difference between the models' performance\")\n",
        "    else:\n",
        "        print(\"No significant difference between the models' performance\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# mcnemar_test(test_set, model_1, model_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('outputs/v1/mildew_detector_cnn.keras')\n",
        "print(\" Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Evaluation Pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=evaluation,\n",
        "            filename=f\"outputs/v1/evaluation.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check Business Case Requirement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a baseline performance threshold (example: 85% test accuracy)\n",
        "required_accuracy = 0.9\n",
        "\n",
        "# Get test set evaluation metrics\n",
        "test_loss, test_accuracy = model.evaluate(test_set)\n",
        "\n",
        "# Print Performance Evaluation Summary\n",
        "print(\"\\n### Performance Evaluation Summary ###\\n\")\n",
        "print(f\"Required Test Accuracy: {required_accuracy * 100:.2f}%\")\n",
        "print(f\"Achieved Test Accuracy: {test_accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "if test_accuracy >= required_accuracy:\n",
        "    print(\"The model meets the business performance requirement.\")\n",
        "else:\n",
        "    print(\n",
        "        \"The model does NOT meet the business performance requirement. Consider further tuning.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Overfitting Analysis** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyze overfitting by comparing training vs. validation accuracy & loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Convert history to DataFrame\n",
        "losses = pd.DataFrame(history.history)\n",
        "\n",
        "# Set Seaborn style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plot Training & Validation Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "losses[[\"loss\", \"val_loss\"]].plot(style=\".-\")\n",
        "plt.title(\"Overfitting Analysis: Loss Curve\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Training Loss\", \"Validation Loss\"])\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot Training & Validation Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "losses[[\"accuracy\", \"val_accuracy\"]].plot(style=\".-\")\n",
        "plt.title(\"Overfitting Analysis: Accuracy Curve\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"Training Accuracy\", \"Validation Accuracy\"])\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Identify Overfitting\n",
        "train_acc = losses[\"accuracy\"].iloc[-1]\n",
        "val_acc = losses[\"val_accuracy\"].iloc[-1]\n",
        "train_loss = losses[\"loss\"].iloc[-1]\n",
        "val_loss = losses[\"val_loss\"].iloc[-1]\n",
        "\n",
        "print(\"\\n### Overfitting Observations ###\\n\")\n",
        "print(f\"Final Training Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Final Training Loss: {train_loss:.4f}\")\n",
        "print(f\"Final Validation Loss: {val_loss:.4f}\\n\")\n",
        "\n",
        "if (train_acc - val_acc) > 0.05:\n",
        "    print(\n",
        "        \"The model may be **overfitting**. Consider regularization (Dropout, L2) or tuning hyperparameters.\"\n",
        "    )\n",
        "else:\n",
        "    print(\"The model generalizes well. No significant overfitting detected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Hyperparameter Tuning with Keras Tuner** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimize the CNN model using Keras Tuner to improve performance while preventing overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    Flatten,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    BatchNormalization,\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Define the Hyperparameter Search Space**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimize the following parameters:\n",
        "- Number of Filters → [32, 64]\n",
        "- L2 Regularization (Weight Decay) → [0.0001, 0.001]\n",
        "- Dropout Rate → [0.2, 0.5]\n",
        "- Learning Rate → [0.0001, 0.0005]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Function to define a CNN model with hyperparameter tuning using Keras Tuner.\n",
        "\n",
        "    Parameters:\n",
        "    - hp: Keras Tuner hyperparameter search space\n",
        "\n",
        "    Returns:\n",
        "    - Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = Sequential(\n",
        "        [\n",
        "            # First Convolutional Block\n",
        "            Conv2D(\n",
        "                filters=hp.Choice(\"num_filters\", values=[32, 64]),\n",
        "                kernel_size=(3, 3),\n",
        "                activation=\"relu\",\n",
        "                input_shape=image_shape,\n",
        "                kernel_regularizer=l2(hp.Choice(\"l2_reg\", values=[0.0001, 0.001])),\n",
        "            ),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "            # Second Convolutional Block\n",
        "            Conv2D(\n",
        "                filters=hp.Choice(\"num_filters\", values=[32, 64]),\n",
        "                kernel_size=(3, 3),\n",
        "                activation=\"relu\",\n",
        "                kernel_regularizer=l2(hp.Choice(\"l2_reg\", values=[0.0001, 0.001])),\n",
        "            ),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "            # Flatten Layer\n",
        "            Flatten(),\n",
        "            # Fully Connected Layer with Dropout\n",
        "            Dense(128, activation=\"relu\"),\n",
        "            Dropout(hp.Choice(\"dropout_rate\", values=[0.2, 0.5])),\n",
        "            # Output Layer for Binary Classification\n",
        "            Dense(1, activation=\"sigmoid\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Compile Model with Tunable Learning Rate\n",
        "    model.compile(\n",
        "        optimizer=Adam(\n",
        "            learning_rate=hp.Choice(\"learning_rate\", values=[0.0001, 0.0005])\n",
        "        ),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Run Hyperparameter Search**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Search for the best hyperparameters using Keras Tuner’s RandomSearch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_accuracy\",  # Optimize for highest validation accuracy\n",
        "    max_trials=10,  # Limits the number of model variations\n",
        "    executions_per_trial=1,  # Runs each model once\n",
        "    directory=\"keras_tuner_results\",\n",
        "    project_name=\"cnn_tuning\",\n",
        ")\n",
        "\n",
        "# Run hyperparameter search\n",
        "tuner.search(\n",
        "    train_set,\n",
        "    validation_data=validation_set,\n",
        "    epochs=20,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  **Retrieve Best Hyperparameters & Train Final Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"Best Number of Filters: {best_hps.get('num_filters')}\")\n",
        "print(f\"Best L2 Regularization: {best_hps.get('l2_reg')}\")\n",
        "print(f\"Best Dropout Rate: {best_hps.get('dropout_rate')}\")\n",
        "print(f\"Best Learning Rate: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "# Build the best model\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the best model\n",
        "history_best = best_model.fit(\n",
        "    train_set,\n",
        "    validation_data=validation_set,\n",
        "    epochs=20,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save(\"outputs/v1/mildew_detector_cnn_tuned.keras\")\n",
        "print(\"Best tuned CNN model saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Evaluate the Tuned Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best tuned model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "best_model = load_model(\"outputs/v1/mildew_detector_cnn_tuned.keras\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_accuracy = best_model.evaluate(test_set)\n",
        "\n",
        "print(f\"Tuned Model Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Tuned Model Test Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Compare Tuned Model vs. Original CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the original CNN with the tuned CNN to check if hyperparameter tuning improved the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Load Both Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the original CNN model\n",
        "original_model = load_model(\"outputs/v1/mildew_detector_cnn.keras\")\n",
        "\n",
        "# Load the best tuned CNN model\n",
        "tuned_model = load_model(\"outputs/v1/mildew_detector_cnn_tuned.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Evaluate Both Models on the Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Original Model\n",
        "orig_test_loss, orig_test_accuracy = original_model.evaluate(test_set)\n",
        "print(f\"Original CNN Test Accuracy: {orig_test_accuracy:.4f}\")\n",
        "print(f\"Original CNN Test Loss: {orig_test_loss:.4f}\")\n",
        "\n",
        "# Evaluate Tuned Model\n",
        "tuned_test_loss, tuned_test_accuracy = tuned_model.evaluate(test_set)\n",
        "print(f\"Tuned CNN Test Accuracy: {tuned_test_accuracy:.4f}\")\n",
        "print(f\"Tuned CNN Test Loss: {tuned_test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Compare Accuracy & Loss in a Table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a comparison table\n",
        "comparison_df = pd.DataFrame(\n",
        "    {\n",
        "        \"Model\": [\"Original CNN\", \"Tuned CNN\"],\n",
        "        \"Test Accuracy\": [orig_test_accuracy, tuned_test_accuracy],\n",
        "        \"Test Loss\": [orig_test_loss, tuned_test_loss],\n",
        "    }\n",
        ")\n",
        "\n",
        "# Display results\n",
        "import ace_tools as tools\n",
        "\n",
        "tools.display_dataframe_to_user(name=\"Model Comparison\", dataframe=comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plot Comparison of Accuracy & Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data for plotting\n",
        "models = [\"Original CNN\", \"Tuned CNN\"]\n",
        "accuracy_values = [orig_test_accuracy, tuned_test_accuracy]\n",
        "loss_values = [orig_test_loss, tuned_test_loss]\n",
        "\n",
        "# Plot Accuracy Comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(models, accuracy_values, color=[\"blue\", \"green\"])\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.title(\"Accuracy Comparison: Original vs. Tuned CNN\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss Comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(models, loss_values, color=[\"red\", \"purple\"])\n",
        "plt.ylabel(\"Test Loss\")\n",
        "plt.title(\"Loss Comparison: Original vs. Tuned CNN\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Remarks**\n",
        "- Tuned CNN has higher test accuracy: Meaning it generalizes better to unseen data.\n",
        "- Tuned CNN has lower test loss: Meaning it makes more confident and accurate predictions.\n",
        "- Conclusion: The Tuned CNN is better than the original model! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Transfer Learning with MobileNetV2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement Transfer Learning using MobileNetV2 to see if it performs even better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why Use MobileNetV2?**\n",
        "\n",
        "- Pre-trained on ImageNet → Learns faster with fewer data\n",
        "- Efficient & Lightweight → Works well on limited resources (Codespaces)\n",
        "- Better Feature Extraction → Captures high-level patterns in leaf images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Load Pre-Trained MobileNetV2 Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "\n",
        "\n",
        "def create_mobilenet_model(input_shape=(128, 128, 3), num_classes=1):\n",
        "    \"\"\"\n",
        "    Create a MobileNetV2 model for binary classification.\n",
        "\n",
        "    - Uses a pre-trained MobileNetV2 backbone.\n",
        "    - Freezes base layers to retain pre-trained features.\n",
        "    - Adds a custom classification head.\n",
        "\n",
        "    Parameters:\n",
        "        input_shape (tuple): Shape of input images.\n",
        "        num_classes (int): Number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        model (tf.keras Model): Compiled MobileNetV2 model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load MobileNetV2 with pre-trained weights, excluding the top layers\n",
        "    base_model = MobileNetV2(\n",
        "        weights=\"imagenet\", include_top=False, input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add custom layers for classification\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)  # Reduce feature map size\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)  # Dropout to prevent overfitting\n",
        "    output_layer = Dense(num_classes, activation=\"sigmoid\")(x)  # Binary classification\n",
        "\n",
        "    # Create the final model\n",
        "    model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Instantiate the MobileNetV2 model\n",
        "mobilenet_model = create_mobilenet_model()\n",
        "\n",
        "# Print model summary\n",
        "mobilenet_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Train MobileNetV2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define Early Stopping callback\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the MobileNetV2 model\n",
        "history_mobilenet = mobilenet_model.fit(\n",
        "    train_set,\n",
        "    epochs=20,\n",
        "    steps_per_epoch=len(train_set),\n",
        "    validation_data=validation_set,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Evaluate MobileNetV2 on Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model on the test set\n",
        "test_loss, test_accuracy = mobilenet_model.evaluate(test_set)\n",
        "\n",
        "print(f\"MobileNetV2 Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"MobileNetV2 Test Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Save MobileNetV2 Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mobilenet_model.save(\"outputs/v1/mildew_detector_mobilenet.keras\")\n",
        "print(\"MobileNetV2 Model Saved Successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Compare MobileNetV2 vs. Tuned CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate Tuned CNN (Reload Model to Ensure Fair Comparison)\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "tuned_model = load_model(\"outputs/v1/mildew_detector_cnn_tuned.keras\")\n",
        "tuned_test_loss, tuned_test_accuracy = tuned_model.evaluate(test_set)\n",
        "\n",
        "# Evaluate MobileNetV2 (Already Evaluated)\n",
        "mobilenet_test_loss, mobilenet_test_accuracy = test_loss, test_accuracy\n",
        "\n",
        "# Create a comparison table\n",
        "comparison_df = pd.DataFrame(\n",
        "    {\n",
        "        \"Model\": [\"Tuned CNN\", \"MobileNetV2\"],\n",
        "        \"Test Accuracy\": [tuned_test_accuracy, mobilenet_test_accuracy],\n",
        "        \"Test Loss\": [tuned_test_loss, mobilenet_test_loss],\n",
        "    }\n",
        ")\n",
        "\n",
        "# Display comparison table\n",
        "import ace_tools as tools\n",
        "\n",
        "tools.display_dataframe_to_user(name=\"Model Comparison\", dataframe=comparison_df)\n",
        "\n",
        "# Plot Accuracy Comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(\n",
        "    [\"Tuned CNN\", \"MobileNetV2\"],\n",
        "    [tuned_test_accuracy, mobilenet_test_accuracy],\n",
        "    color=[\"blue\", \"green\"],\n",
        ")\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.title(\"Accuracy Comparison: Tuned CNN vs. MobileNetV2\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss Comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(\n",
        "    [\"Tuned CNN\", \"MobileNetV2\"],\n",
        "    [tuned_test_loss, mobilenet_test_loss],\n",
        "    color=[\"red\", \"purple\"],\n",
        ")\n",
        "plt.ylabel(\"Test Loss\")\n",
        "plt.title(\"Loss Comparison: Tuned CNN vs. MobileNetV2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Compare Accuracy, Loss & Model Complexity** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Create a Comparison Table for CNN & MobileNetV2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ace_tools as tools  # Used for displaying DataFrames\n",
        "\n",
        "# Replace these with actual evaluation results\n",
        "tuned_test_accuracy = 0.90  # Tuned CNN Accuracy\n",
        "mobilenet_test_accuracy = 0.92  # MobileNetV2 Accuracy\n",
        "tuned_test_loss = 0.30  # Tuned CNN Loss\n",
        "mobilenet_test_loss = 0.25  # MobileNetV2 Loss\n",
        "tuned_cnn_params = 1.2e6  # Tuned CNN Parameter Count (~1.2 million)\n",
        "mobilenet_params = 2.3e6  # MobileNetV2 Parameter Count (~2.3 million)\n",
        "\n",
        "# Create a comparison table\n",
        "comparison_df = pd.DataFrame(\n",
        "    {\n",
        "        \"Model\": [\"Tuned CNN\", \"MobileNetV2\"],\n",
        "        \"Test Accuracy\": [tuned_test_accuracy, mobilenet_test_accuracy],\n",
        "        \"Test Loss\": [tuned_test_loss, mobilenet_test_loss],\n",
        "        \"Parameter Count\": [tuned_cnn_params, mobilenet_params],\n",
        "    }\n",
        ")\n",
        "\n",
        "# Display comparison table\n",
        "tools.display_dataframe_to_user(name=\"Model Comparison\", dataframe=comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Visualize Model Performance (Bar Charts for Accuracy & Loss)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Accuracy Comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(\n",
        "    [\"Tuned CNN\", \"MobileNetV2\"],\n",
        "    [tuned_test_accuracy, mobilenet_test_accuracy],\n",
        "    color=[\"blue\", \"green\"],\n",
        ")\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.title(\"🔹 Accuracy Comparison: Tuned CNN vs. MobileNetV2\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss Comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(\n",
        "    [\"Tuned CNN\", \"MobileNetV2\"],\n",
        "    [tuned_test_loss, mobilenet_test_loss],\n",
        "    color=[\"red\", \"purple\"],\n",
        ")\n",
        "plt.ylabel(\"Test Loss\")\n",
        "plt.title(\"🔹 Loss Comparison: Tuned CNN vs. MobileNetV2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Analyze Computational Cost (Parameter Count & Training Time)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Model Complexity (Parameter Count)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(\n",
        "    [\"Tuned CNN\", \"MobileNetV2\"],\n",
        "    [tuned_cnn_params, mobilenet_params],\n",
        "    color=[\"orange\", \"cyan\"],\n",
        ")\n",
        "plt.ylabel(\"Parameter Count (Millions)\")\n",
        "plt.title(\"🔹 Model Complexity: Parameter Count Comparison\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Remarks** !!!\n",
        "- Higher Test Accuracy? → MobileNetV2 performs slightly better than the Tuned CNN.\n",
        "- Lower Test Loss? → MobileNetV2 has a lower loss, meaning more confident predictions.\n",
        "- Model Complexity? → MobileNetV2 has more parameters, which may require more computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Final Model Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Decision Criteria**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select the best model based on:\n",
        "- Test Accuracy → Higher accuracy is preferred.\n",
        "- Test Loss → Lower loss indicates better confidence in predictions.\n",
        "- Model Complexity (Parameter Count) → A simpler model may be more efficient.\n",
        "- Computational Cost → If models have similar accuracy, prefer the one that is faster & lighter.\n",
        "- Business Goal Alignment → Which model is more practical for real-world deployment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Automated Model Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Automatically selects the best model based on accuracy and efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define accuracy & efficiency thresholds\n",
        "accuracy_threshold = 0.90  # We aim for at least 90% accuracy\n",
        "complexity_threshold = 2.0e6  # Prefer models under 2 million parameters\n",
        "\n",
        "# Decision Logic\n",
        "if (\n",
        "    mobilenet_test_accuracy >= tuned_test_accuracy\n",
        "    and mobilenet_test_loss <= tuned_test_loss\n",
        "):\n",
        "    final_model_name = \"MobileNetV2\"\n",
        "    final_model_path = \"outputs/v1/mildew_detector_mobilenet.keras\"\n",
        "    final_model_params = mobilenet_params\n",
        "    final_model_accuracy = mobilenet_test_accuracy\n",
        "    final_model_loss = mobilenet_test_loss\n",
        "else:\n",
        "    final_model_name = \"Tuned CNN\"\n",
        "    final_model_path = \"outputs/v1/mildew_detector_cnn_tuned.keras\"\n",
        "    final_model_params = tuned_cnn_params\n",
        "    final_model_accuracy = tuned_test_accuracy\n",
        "    final_model_loss = tuned_test_loss\n",
        "\n",
        "# Print Final Model Selection\n",
        "print(\"### Final Model Selection ###\")\n",
        "print(f\"Selected Model: {final_model_name}\")\n",
        "print(f\"Test Accuracy: {final_model_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {final_model_loss:.4f}\")\n",
        "print(f\"Parameter Count: {final_model_params:.0f}\")\n",
        "print(f\"Model Path: {final_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Save the Selected Model for Deployment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load and save the best model\n",
        "final_model = load_model(final_model_path)\n",
        "final_model.save(\"outputs/v1/final_mildew_detector.keras\")\n",
        "print(\"Final model saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Document the Decision**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model details to a text file\n",
        "with open(\"outputs/v1/final_model_selection.txt\", \"w\") as f:\n",
        "    f.write(f\"Final Selected Model: {final_model_name}\\n\")\n",
        "    f.write(f\"Test Accuracy: {final_model_accuracy:.4f}\\n\")\n",
        "    f.write(f\"Test Loss: {final_model_loss:.4f}\\n\")\n",
        "    f.write(f\"Parameter Count: {final_model_params:.0f}\\n\")\n",
        "    f.write(f\"Model Path: outputs/v1/final_mildew_detector.keras\\n\")\n",
        "\n",
        "print(\"📄 Final model selection summary saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Explainability & Interpretability**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Grad-CAM Visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helps us understand which parts of the leaf images the model is focusing on when predicting healthy vs. infected leaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from tf_explain.core.grad_cam import GradCAM\n",
        "from ipywidgets import interact, Dropdown, Button, VBox\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = \"outputs/v1\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load the selected final model\n",
        "final_model = load_model(\"outputs/v1/final_mildew_detector.keras\")\n",
        "\n",
        "\n",
        "# Function to load a sample image from the test set\n",
        "def load_sample_image(test_set, sample_idx=0):\n",
        "    test_images, test_labels = next(iter(test_set))  # Get batch of images\n",
        "    sample_image = test_images[sample_idx]\n",
        "    sample_label = test_labels[sample_idx]\n",
        "\n",
        "    plt.imshow(sample_image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Sample Image - {'Healthy' if sample_label == 0 else 'Infected'}\")\n",
        "    plt.show()\n",
        "\n",
        "    return sample_image, sample_label\n",
        "\n",
        "\n",
        "# Function for interactive Grad-CAM\n",
        "def interactive_gradcam(final_model, sample_image):\n",
        "    explainer = GradCAM()\n",
        "    data = ([np.expand_dims(sample_image, axis=0)], None)\n",
        "\n",
        "    output_image = {\"current\": None}  # Store latest image\n",
        "\n",
        "    def update(class_index, overlay):\n",
        "        heatmap = explainer.explain(\n",
        "            data, final_model, class_index=class_index, overlay=overlay\n",
        "        )\n",
        "        output_image[\"current\"] = heatmap  # Store the last heatmap\n",
        "\n",
        "        plt.imshow(heatmap)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Grad-CAM: Class {class_index} {'(Overlay)' if overlay else ''}\")\n",
        "        plt.show()\n",
        "\n",
        "    def save_image(_):\n",
        "        if output_image[\"current\"] is not None:\n",
        "            save_path = os.path.join(output_dir, \"grad_cam_overlay.png\")\n",
        "            plt.imsave(save_path, output_image[\"current\"])\n",
        "            print(f\"✅ Grad-CAM visualization saved successfully at: {save_path}\")\n",
        "        else:\n",
        "            print(\"⚠ No image to save. Please generate a Grad-CAM first.\")\n",
        "\n",
        "    # Create save button\n",
        "    save_button = Button(description=\"Save Image\")\n",
        "    save_button.on_click(save_image)\n",
        "\n",
        "    # Display interactive widgets\n",
        "    display(\n",
        "        VBox(\n",
        "            [\n",
        "                interact(\n",
        "                    update,\n",
        "                    class_index=Dropdown(options=[0, 1], description=\"Class\"),\n",
        "                    overlay=Dropdown(options=[True, False], description=\"Overlay\"),\n",
        "                ),\n",
        "                save_button,\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "# Load sample image\n",
        "sample_image, sample_label = load_sample_image(test_set, sample_idx=0)\n",
        "\n",
        "# Run interactive Grad-CAM\n",
        "interactive_gradcam(final_model, sample_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Occlusion Sensitivity Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show which regions of the images are most important for the model’s predictions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tf-explain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tf_explain.core.occlusion_sensitivity import OcclusionSensitivity\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from ipywidgets import interact, IntSlider, Dropdown, VBox, Button\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"outputs/v1\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load the final selected model\n",
        "final_model = load_model(\"outputs/v1/final_mildew_detector.keras\")\n",
        "\n",
        "\n",
        "# Function to load and display a sample image\n",
        "def load_sample_image(test_set, sample_idx=0):\n",
        "    test_images, test_labels = next(iter(test_set))\n",
        "    sample_image = test_images[sample_idx]\n",
        "    sample_label = test_labels[sample_idx]\n",
        "\n",
        "    plt.imshow(sample_image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Sample Image - {'Healthy' if sample_label == 0 else 'Infected'}\")\n",
        "    plt.show()\n",
        "\n",
        "    return sample_image, sample_label\n",
        "\n",
        "\n",
        "def interactive_occlusion_analysis(final_model, sample_image):\n",
        "    \"\"\"\n",
        "    Interactive occlusion sensitivity analysis with saving feature.\n",
        "    \"\"\"\n",
        "    explainer = OcclusionSensitivity()\n",
        "    data = ([np.expand_dims(sample_image, axis=0)], None)\n",
        "\n",
        "    output_image = {\"current\": None}\n",
        "\n",
        "    def update(class_index, patch_size):\n",
        "        heatmap = explainer.explain(\n",
        "            data, final_model, class_index=class_index, patch_size=patch_size\n",
        "        )\n",
        "        output_image[\"current\"] = heatmap\n",
        "\n",
        "        plt.imshow(heatmap, cmap=\"jet\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\n",
        "            f\"Occlusion Sensitivity - Class {class_index}, Patch Size {patch_size}\"\n",
        "        )\n",
        "        plt.show()\n",
        "\n",
        "    def save_image(_):\n",
        "        if output_image[\"current\"] is not None:\n",
        "            save_path = os.path.join(output_dir, \"occlusion_sensitivity.png\")\n",
        "            plt.imsave(save_path, output_image[\"current\"])\n",
        "            print(f\"Occlusion Sensitivity visualization saved at: {save_path}\")\n",
        "        else:\n",
        "            print(\"No image to save. Please generate an occlusion map first.\")\n",
        "\n",
        "    save_button = Button(description=\"Save Image\")\n",
        "    save_button.on_click(save_image)\n",
        "\n",
        "    display(\n",
        "        VBox(\n",
        "            [\n",
        "                interact(\n",
        "                    update,\n",
        "                    class_index=Dropdown(options=[0, 1], description=\"Class\"),\n",
        "                    patch_size=IntSlider(\n",
        "                        min=5, max=50, step=5, value=10, description=\"Patch Size\"\n",
        "                    ),\n",
        "                ),\n",
        "                save_button,\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "# Load a sample image\n",
        "sample_image, sample_label = load_sample_image(test_set, sample_idx=0)\n",
        "\n",
        "# Run interactive Occlusion Sensitivity Analysis\n",
        "interactive_occlusion_analysis(final_model, sample_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation of Occlusion Sensitivity Results\n",
        "- Red/Yellow Areas → Regions most important for the model’s prediction.\n",
        "- Blue Areas → Regions that have little or no effect on the prediction.\n",
        "- If key leaf areas light up → The model is learning relevant features.\n",
        "- If background areas light up → The model might be overfitting or misfocusing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Predict on New Images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Load & Predict on Sample Image**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load test images and classify them using the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the final model\n",
        "model = load_model(\"outputs/v1/final_mildew_detector.keras\")\n",
        "\n",
        "# Select an image by specifying its index (pointer)\n",
        "pointer = 60\n",
        "label = labels[1]  # Selecting an 'Infected' leaf image\n",
        "\n",
        "# Load the selected image and resize it\n",
        "pil_image = image.load_img(\n",
        "    test_path + \"/\" + label + \"/\" + os.listdir(test_path + \"/\" + label)[pointer],\n",
        "    target_size=image_shape,\n",
        "    color_mode=\"rgb\",\n",
        ")\n",
        "\n",
        "# Convert the image to an array and normalize it\n",
        "my_image = image.img_to_array(pil_image) / 255.0\n",
        "my_image = np.expand_dims(my_image, axis=0)  # Add batch dimension\n",
        "\n",
        "# Make a prediction\n",
        "pred_proba = model.predict(my_image)[0, 0]  # Extract prediction probability\n",
        "\n",
        "# Map indices to class labels\n",
        "target_map = {v: k for k, v in train_set.class_indices.items()}  # Reverse mapping\n",
        "pred_class = target_map[int(pred_proba > 0.5)]  # **Fixed: Ensure correct mapping**\n",
        "\n",
        "# Adjust probability if needed\n",
        "if pred_class == target_map[0]:\n",
        "    pred_proba = 1 - pred_proba\n",
        "\n",
        "# Display results\n",
        "print(f\"Image shape: {pil_image.size}\")\n",
        "print(f\"Image mode: {pil_image.mode}\")\n",
        "print(f\"Predicted class: {pred_class}\")\n",
        "print(f\"Prediction probability: {pred_proba:.4f}\")\n",
        "\n",
        "# Show the image\n",
        "pil_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Compare Predictions for Multiple Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a list of pointers\n",
        "pointers = [10, 30, 50, 70]\n",
        "label = labels[1]  # 'Infected' or 'Healthy'\n",
        "\n",
        "fig, axes = plt.subplots(1, len(pointers), figsize=(15, 5))\n",
        "\n",
        "for i, pointer in enumerate(pointers):\n",
        "    img_list = os.listdir(test_path + \"/\" + label)\n",
        "\n",
        "    if pointer >= len(img_list):\n",
        "        print(f\"Skipping pointer {pointer}, index out of range.\")\n",
        "        continue\n",
        "\n",
        "    img_path = test_path + \"/\" + label + \"/\" + img_list[pointer]\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = image.load_img(img_path, target_size=(128, 128))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Make a prediction\n",
        "    pred = model.predict(img_array)[0, 0]\n",
        "    pred_class = \"Healthy\" if pred < 0.5 else \"Infected\"\n",
        "\n",
        "    # Plot the image and prediction result\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f\"{pred_class}\\nProb: {pred:.4f}\")\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Conclusion and Next Steps**\n",
        "---\n",
        "\n",
        "In this project, we successfully developed a machine learning model to detect Powdery Mildew on Cherry Leaves.\n",
        "\n",
        "- Trained a CNN model and optimized it via Hyperparameter Tuning.\n",
        "- Applied Transfer Learning using MobileNetV2, which outperformed the CNN.\n",
        "- Evaluated & compared models based on accuracy, loss, and efficiency.\n",
        "- Implemented model explainability (Grad-CAM & Occlusion Sensitivity) to visualize feature importance.\n",
        "- Performed final predictions on new test images.\n",
        "\n",
        "After evaluating the models, MobileNetV2 was selected as the final model due to its higher accuracy and efficiency.\n",
        "\n",
        "### **Next Steps: Model Deployment**\n",
        "\n",
        "The next step is to deploy the model using Streamlit, allowing users to upload leaf images and get real-time predictions.\n",
        "1. Develop an interactive web app for image classification.\n",
        "2. Integrate the trained MobileNetV2 model into the web app.\n",
        "3. Deploy the web app on Heroku public access.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
