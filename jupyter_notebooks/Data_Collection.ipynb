{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **DATA COLLECTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## **Objectives**\n",
        "\n",
        "1. Import necessary packages and configure the working directory.\n",
        "2. Authenticate and retrieve the mildew dataset from Kaggle.\n",
        "3. Prepare the dataset by organizing it into train, validation, and test splits.\n",
        "4. Ensure data integrity by removing any non-image files.\n",
        "\n",
        "## **Inputs**\n",
        "\n",
        "- Kaggle JSON file: Used for authentication and dataset download.\n",
        "- Dataset sourse: The mildew dataset hosted on Kaggle.\n",
        "- Local directories: Structure for storing and splitting data.\n",
        "\n",
        "## **Outputs**\n",
        "\n",
        "- Raw Dataset: Downloaded and unzipped into the specified folder\n",
        "- Cleaned Dataset: Non-image files removed for consistency.\n",
        "- Structured Data: Split into training (70%), validation (10%), and testing (20%) sets, organized in respective directories.\n",
        "\n",
        "## **Additional Comments**\n",
        "\n",
        "- These steps are critical to ensure the dataset is properly prepared for model training and evaluation. By structuring and cleaning the data, we minimize errors during training and improve model accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## **Change Working Directory**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Import Necessary Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Set Working Directory & File Paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspaces/mildew-detection-app/jupyter_notebooks'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/workspaces/mildew-detection-app')\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the New Current Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspaces/mildew-detection-app'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "### Install Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /home/codespace/.local/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /home/codespace/.local/lib/python3.12/site-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /home/codespace/.local/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from kaggle) (2.32.3)\n",
            "Collecting tqdm (from kaggle)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting python-slugify (from kaggle)\n",
            "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: urllib3 in /home/codespace/.local/lib/python3.12/site-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /home/codespace/.local/lib/python3.12/site-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /home/codespace/.local/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\n",
            "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
            "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kaggle) (3.10)\n",
            "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105839 sha256=22327282211a27cb900adc16eb2ea8431040cd02e21b680d15cbf97972f1b375\n",
            "  Stored in directory: /home/codespace/.cache/pip/wheels/46/d2/26/84d0a1acdb9c6baccf7d28cf06962ec80529fe1ad938489983\n",
            "Successfully built kaggle\n",
            "Installing collected packages: text-unidecode, tqdm, python-slugify, kaggle\n",
            "Successfully installed kaggle-1.6.17 python-slugify-8.0.4 text-unidecode-1.3 tqdm-4.67.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the Kaggle configuration directory to the current working directory and set permissions for the Kaggle authentication JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Dataset and Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/codeinstitute/cherry-leaves\n",
            "License(s): unknown\n",
            "Downloading cherry-leaves.zip to inputs/mildew_dataset\n",
            " 93%|███████████████████████████████████▏  | 51.0M/55.0M [00:02<00:00, 29.8MB/s]\n",
            "100%|██████████████████████████████████████| 55.0M/55.0M [00:02<00:00, 21.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "KaggleDatasetPath = \"codeinstitute/cherry-leaves\"\n",
        "DestinationFolder = \"inputs/mildew_dataset\"   \n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unzip the Downloaded File and Delete the ZIP File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(DestinationFolder + '/cherry-leaves.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(DestinationFolder)\n",
        "\n",
        "os.remove(DestinationFolder + '/cherry-leaves.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## **Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_image_file(my_data_dir):\n",
        "    \"\"\"\n",
        "    Remove files that are not images from the dataset directory. \n",
        "    \"\"\"\n",
        "    image_extension = ('.png', '.jpg', '.jpeg')\n",
        "    folders = os.listdir(my_data_dir)\n",
        "    for folder in folders:\n",
        "        files = os.listdir(my_data_dir + '/' + folder)\n",
        "        # print(files)\n",
        "        i = []\n",
        "        j = []\n",
        "        for given_file in files:\n",
        "            if not given_file.lower().endswith(image_extension):\n",
        "                file_location = my_data_dir + '/' + folder + '/' + given_file\n",
        "                os.remove(file_location)  \n",
        "                i.append(1)\n",
        "            else:\n",
        "                j.append(1)\n",
        "                pass\n",
        "        print(f\"Folder: {folder} - has image file\", len(j))\n",
        "        print(f\"Folder: {folder} - has non-image file\", len(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder: Healthy - has image file 2104\n",
            "Folder: Healthy - has non-image file 0\n",
            "Folder: Infected - has image file 2104\n",
            "Folder: Infected - has non-image file 0\n"
          ]
        }
      ],
      "source": [
        "remove_non_image_file(my_data_dir='inputs/mildew_dataset/cherry-leaves')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Detect and Remove Corrupt Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total corrupt images found: 0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def check_corrupt_images(directory):\n",
        "    corrupt_images = []\n",
        "\n",
        "    for folder in os.listdir(directory):\n",
        "        folder_path = os.path.join(directory, folder)\n",
        "        if os.path.isdir(folder_path):  # Ensure it's a directory\n",
        "            for img_name in os.listdir(folder_path):\n",
        "                img_path = os.path.join(folder_path, img_name)\n",
        "                try:\n",
        "                    img = Image.open(img_path)  # Try opening the image\n",
        "                    img.verify()  # Verify image integrity\n",
        "                except (IOError, SyntaxError):\n",
        "                    print(f\"Corrupt image detected: {img_path}\")\n",
        "                    corrupt_images.append(img_path)\n",
        "\n",
        "    return corrupt_images\n",
        "\n",
        "# Define your dataset directory\n",
        "dataset_path = \"inputs/mildew_dataset/cherry-leaves\"\n",
        "\n",
        "# Run check on the extracted dataset (Healthy & Infected folders)\n",
        "corrupt_images = check_corrupt_images(dataset_path)\n",
        "\n",
        "print(f\"Total corrupt images found: {len(corrupt_images)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Split Data into Train, Validation, and Test Sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "def split_train_validation_test_images(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio):    \n",
        "    \"\"\"\n",
        "    Split the dataset into training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    # Validate that the sum of train, validation, and test ratios equals 1.0\n",
        "    if train_set_ratio + validation_set_ratio + test_set_ratio!= 1.0:\n",
        "        print(\"Error: train_set_ratio + validation_set_ratio + test_set_ratio should sum to 1.0\")\n",
        "        return\n",
        "\n",
        "    # Get the class labels in the dataset directory \n",
        "    labels = os.listdir(my_data_dir)  \n",
        "\n",
        "    # Create 'train', 'validation', and 'test' folders with class subfolders\n",
        "    for folder in ['train', 'validation', 'test']:\n",
        "        for label in labels:\n",
        "            os.makedirs(name=os.path.join(my_data_dir, folder, label), exist_ok=True)\n",
        "\n",
        "    # Iterate through each class label\n",
        "    for label in labels:\n",
        "        # Get the list of files in the current class label directory\n",
        "        files = os.listdir(os.path.join(my_data_dir, label))\n",
        "        random.shuffle(files)\n",
        "        # Calculate the number of files for train, validation, and test sets\n",
        "        train_set_files_qty = int(len(files) * train_set_ratio)\n",
        "        validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
        "\n",
        "        count = 1\n",
        "        for file_name in files:\n",
        "            if count <= train_set_files_qty:\n",
        "                # Move the file to the 'train' set\n",
        "                shutil.move(os.path.join(my_data_dir, label, file_name),\n",
        "                            os.path.join(my_data_dir, 'train', label, file_name))\n",
        "\n",
        "            elif count <= (train_set_files_qty + validation_set_files_qty):\n",
        "                # Move the file to the 'validation' set\n",
        "                shutil.move(os.path.join(my_data_dir, label, file_name),\n",
        "                            os.path.join(my_data_dir, 'validation', label, file_name))\n",
        "            else:\n",
        "                # Move the file to the 'test' set\n",
        "                shutil.move(os.path.join(my_data_dir, label, file_name),\n",
        "                            os.path.join(my_data_dir, 'test', label, file_name))\n",
        "\n",
        "            count += 1\n",
        "        # Remove the original class directory after all files are moved\n",
        "        os.rmdir(os.path.join(my_data_dir, label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_train_validation_test_images(my_data_dir=f\"inputs/mildew_dataset/cherry-leaves\",\n",
        "                                   train_set_ratio=0.7,\n",
        "                                   validation_set_ratio=0.1,\n",
        "                                   test_set_ratio=0.2\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Count Images in Each Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1472 images in train/Healthy\n",
            "There are 1472 images in train/Infected\n",
            "There are 422 images in test/Healthy\n",
            "There are 422 images in test/Infected\n",
            "There are 210 images in validation/Healthy\n",
            "There are 210 images in validation/Infected\n",
            "\n",
            "Total number of images: 4208\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "sets = ['train', 'test', 'validation']\n",
        "labels = ['Healthy', 'Infected']  \n",
        "\n",
        "for set_name in sets:\n",
        "    for label in labels:\n",
        "        path = f'inputs/mildew_dataset/cherry-leaves/{set_name}/{label}' \n",
        "        try:\n",
        "            number_of_files = len(os.listdir(path))\n",
        "            print(f'There are {number_of_files} images in {set_name}/{label}')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Directory '{path}' not found.\")\n",
        "\n",
        "# Compute total number of images across all datasets (train, validation, test)\n",
        "total_images = 0\n",
        "for set_name in sets:\n",
        "    for label in labels:\n",
        "        path = f'inputs/mildew_dataset/cherry-leaves/{set_name}/{label}'  \n",
        "        try:\n",
        "            total_images += len(os.listdir(path))\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "print(f\"\\nTotal number of images: {total_images}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Conclusion and Next Steps**\n",
        "---\n",
        "\n",
        "This notebook successfully handled **data collection and preprocessing** for the **powdery mildew detection project**. The dataset was:\n",
        "\n",
        "- Downloaded from Kaggle using authentication.\n",
        "- Cleaned by removing non-image files.\n",
        "- Organized into train (70%), validation (10%), and test (20%) sets.\n",
        "- Verified for integrity by counting images in each set.\n",
        "\n",
        "### **Next Steps**:\n",
        "\n",
        "1. Data Exploration & Visualization:\n",
        "\n",
        "- Analyze class distributions and dataset balance.\n",
        "- Generate image samples to check quality.\n",
        "- Compute image dimensions for standardization.\n",
        "\n",
        "2. Data Augmentation & Preprocessing:\n",
        "\n",
        "- Implement augmentation strategies (e.g., rotation, flipping).\n",
        "- Normalize images for deep learning models.\n",
        "\n",
        "3. Model Training & Evaluation:\n",
        "\n",
        "- Utilize the structured dataset for CNN training.\n",
        "- Tune hyperparameters to improve model performance."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
